# real-time-smile-posture-clap-detector (CV)

Pet‑проект для мониторинга состояния человека перед камерой в реальном времени: улыбка,  открытость глаз, осанка, поднятые руки и хлопки.  
Основная идея — использовать геометрию по ключевым точкам (landmarks), а не тяжелые классификаторы, чтобы всё работало на CPU с нормальным FPS.


## Основные возможности

- Детекция улыбки и зевка по геометрическим метрикам рта (MAR, расстояние от центра губ до линии уголков).
- Определение открытых/закрытых глаз по Eye Aspect Ratio (EAR).
- Детекция сутулости и наклона плеч с использованием 3D‑координат (ось Z) из MediaPipe Pose.
- Определение поднятых рук (левая/правая/обе, XOR‑поднятие).
- Подсчёт хлопков руками с помощью простого конечного автомата (State Machine).
- Визуализация: скелет тела, ключевые точки лица, текстовые статусы (SMILE / NO SMILE, POSTURE, CLAPS и т.д.) поверх видеопотока.


## Технологический стек

- **Язык**: Python 3.10.19
- **Библиотеки CV и математики**:
  - OpenCV (*cv2*) — захват видео, отрисовка, базовая обработка изображений.
  - NumPy — векторная алгебра и вычисления.
- **Модели / фреймворки**:
  - MediaPipe FaceMesh — 3D‑ландмарки лица (улыбка, глаза, рот, уши).
  - MediaPipe Pose — 3D‑ландмарки тела (плечи, локти, запястья, бёдра, уши).
  - dlib — детектор лиц и предсказатель 68 ключевых точек.
  - MTCNN (через *facenet-pytorch* и *mtcnn*) — детекция лица и 5 ключевых точек (глаза, нос, уголки рта).
- **Системные библиотеки**:
  - *urllib.request*, *os* — загрузка моделей (каскады Haar, shape_predictor) при первом запуске.
  - *collections.deque* — скользящее окно для сглаживания метрик и истории состояний.



## Пайплайны обработки (по моделям)

### 1. MediaPipe‑pipeline (FaceMesh + Pose)

1. Захват кадра с веб‑камеры (**cv2.VideoCapture**), разворот зеркалом (**cv2.flip**), перевод в RGB.
2. Прогон через **face_mesh.process(...)**:
   - получение 3D‑точек лица;
   - расчёт MAR, геометрической метрики улыбки (**smile_geo_metric**), поднятия уголков рта (**mouth_corner_lift**), статуса глаз (**are_eyes_open**).
3. Прогон через ***pose.process(...)***:
   - получение 3D‑точек тела;
   - определение поднятых рук (**is_hand_raised**);
   - определение осанки (**check_posture**);
   - детекция хлопков (**detect_clap**).
4. Визуализация:
   - текстовые статусы (улыбка, глаза, руки, осанка, количество хлопков);
   - скелет позы и ключевые точки лица.
5. Цикл по кадрам до нажатия **ESC**.

### 2. Dlib‑pipeline (Dlib Face + MediaPipe Pose)

1. Захват кадра, разворот, перевод в RGB.
2. Детекция лиц через **dlib.get_frontal_face_detector()**.
3. Для первого лица:
   - предсказание 68 ландмарок **shape_predictor**,
   - преобразование в NumPy‑массив.
4. Прогон результата через:
   - **SmileDetector.update(...)** — статус улыбки;
   - **are_eyes_open(...)** — статус глаз.
5. Параллельно:
   - **pose_detector.process(...)** — MediaPipe Pose;
   - **PostureDetector.update(...)** — статус осанки;
   - **ClapDetector.update(...)** — счётчик хлопков;
   - **is_hand_raised(...)** — поднятие рук.
6. Отрисовка 68‑точечной маски лица, позы и HUD со всеми статусами.

### 3. MTCNN + MediaPipe‑pipeline (гибрид)

1. Захват кадра, разворот, перевод в RGB.
2. MediaPipe Pose:
   - детекция позы;
   - определение поднятых рук, осанки, хлопков;
   - отрисовка скелета и линии между запястьями.
3. MTCNN + FaceMesh:
   - попытка получить лицо и ландмарки FaceMesh;
   - если FaceMesh есть — детекция улыбки и состояния глаз по плотной сетке;
   - если FaceMesh нет — попытка использовать ландмарки MTCNN (Mouth Width Ratio, относительное поднятие уголков рта).
4. Отрисовка рамок лица, ключевых точек MTCNN/FaceMesh и текстовых метрик (d, lift, MAR и т.п.).


## Структура и основные классы

Проект состоит из трёх логических блоков: **MediaPipe‑пайплайн**, **Dlib‑пайплайн** и **MTCNN‑/MediaPipe‑гибрид**. 

### 1. MediaPipe‑блок (FaceMesh + Pose)

Реализован набором функций и глобальных настроек:

- Геометрические функции:
  - ***mouth_mar(...)*** — считает Mouth Aspect Ratio по внутренним точкам рта (высота / ширина).
  - ***mouth_corner_lift(...)*** — вычисляет относительное поднятие уголков рта.
  - ***smile_geo_metric(...)*** — расстояние от центра верхней губы до прямой, соединяющей уголки рта (нормированное).
  - ***are_eyes_open(...)*** — считает EAR для обоих глаз и решает, открыты ли глаза.
- Постура и жесты:
  - ***check_posture(pose_lms, face_lms, w, h)*** — определяет статус осанки: *GOOD*, *SLOUCHED*, *TILTED*, *UNKNOWN* по Z‑координате ушей и плеч и разнице по Y.
  - ***is_hand_raised(pose_lms, left=True/False)*** — рука поднята, если запястье заметно выше плеча, при достаточной видимости точек.
  - ***detect_clap(pose_lms, current_clap_state, current_clap_count)*** — конечный автомат по расстоянию между запястьями в нормализованных координатах.
- Визуализация:
  - ***annotate_frame(...)*** — рисует на кадре:
    - текстовые статусы (улыбка, глаза, руки, осанка, количество хлопков),
    - линии рта и ключевые точки лица,
    - схему скелета (плечи, локти, запястья, бёдра, уши) с цветовой индикацией событий.

### 2. Dlib‑блок (68‑landmarks + MediaPipe Pose)

Основная логика разнесена по классам:

- ***SmileDetector*** (Dlib‑версия):
  - Работает по 68 ключевым точкам лица (***shape_predictor_68_face_landmarks.dat***).
  - Считает:
    - расстояние от центра губ до линии ротовых уголков (**d**),
    - вертикальное поднятие/опускание губ (**lift**),
    - MAR по губам (**mar**),
    - отношение ширины рта к межглазному расстоянию (**mwr**).
  - Хранит историю метрик в **deque**, сглаживает медианой, отдельно ведёт “базовую линию” спокойно/нейтрального лица, чтобы детектировать улыбку относительно персональной нормы.
- **PostureDetector** (Pose‑Z):
  - Принимает **pose_landmarks** от MediaPipe Pose.
  - В буфере хранит историю **z_diff** — разницы средней глубины плеч и ушей.
  - Возвращает статус **GOOD / SLOUCHED / TILTED / WAITING... / UNKNOWN** и цвет для отрисовки.
- **ClapDetector** (Dlib‑секция):
  - Использует абсолютное расстояние между запястьями в пикселях.
  - Имеет **cooldown**, чтобы не засчитывать один хлопок несколько раз.

Дополнительные функции:

- **are_eyes_open(lms, threshold=0.2)** — EAR по индексам Dlib (36–47).
- **draw_landmarks(frame, face_lms, pose_lms, posture_color)** — рисует 68 точек и позу.
- **draw_final_status(frame, fps, states)** — финальный HUD: улыбка, глаза, осанка, поднятые руки, количество хлопков.

### 3. MTCNN + MediaPipe‑блок (гибрид)

Здесь комбинируются MTCNN/FaceMesh для лица и MediaPipe Pose для тела.

- Обёртки над детекторами:
  - **init_mtcnn()** — инициализирует MTCNN из **facenet-pytorch** и **mtcnn**.
  - **mtcnn_detect(fpt, ipz, frame_bgr)** — возвращает список детекций с боксами и ключевыми точками (глаза, нос, рот).
  - **draw_mtcnn_detections(frame, detections)** — рисует рамки и ключевые точки от MTCNN.
- Геометрия по FaceMesh:
  - **is_smile_facemesh(lms, w, h)** — объединяет несколько условий (геометрический **d**, **lift**, **mar**) для детекции улыбки и отсечки зевка.
  - **are_eyes_open(...)** — EAR по индексам MediaPipe.
- Жесты и осанка:
  - **PostureDetector** (аналогичный dlib‑версии, но с другими порогами).
  - **ClapDetector** (расстояние между запястьями, нормализованное на ширину плеч + гистерезис).
  - **is_hand_raised(...)**, **annotate_pose(...)**, **annotate_clap(...)** — отрисовка скелета, запястий, линии между руками и метрик хлопка.




