{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b091a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import urllib.request\n",
    "import os\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c482a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"geo\"\n",
    "cam_index = 0\n",
    "smile_thresh_geo = 0.04\n",
    "smile_thresh_mar = 0.34\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "face_cascade_filename = \"haarcascade_frontalface_default.xml\"\n",
    "eye_cascade_filename = \"haarcascade_eye.xml\"\n",
    "smile_cascade_filename = \"haarcascade_smile.xml\"\n",
    "\n",
    "base_urls = [\n",
    "    \"https://github.com/opencv/opencv/raw/4.x/data/haarcascades/\",\n",
    "    \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/\"\n",
    "]\n",
    "\n",
    "def download_if_needed(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return True\n",
    "    for base in base_urls:\n",
    "        url = base + filename\n",
    "        try:\n",
    "            print(f\"Файл '{filename}' не найден, скачивает из {url}\")\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            print(\"Файл скачан\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось скачать из {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "def load_cascade(filename):\n",
    "    if os.path.exists(filename):\n",
    "        cas = cv2.CascadeClassifier(filename)\n",
    "        if not cas.empty():\n",
    "            return cas\n",
    "        else:\n",
    "            print(f\"Внимание: файл {filename} есть, но загрузка не удалась (возможен битый файл)\")\n",
    "    try:\n",
    "        haar_dir = cv2.data.haarcascades\n",
    "        path = os.path.join(haar_dir, filename)\n",
    "        cas = cv2.CascadeClassifier(path)\n",
    "        if not cas.empty():\n",
    "            print(f\"Загружено из cv2.data.haarcascades: {path}\")\n",
    "            return cas\n",
    "    except Exception:\n",
    "        pass\n",
    "    return cv2.CascadeClassifier() \n",
    "\n",
    "download_if_needed(face_cascade_filename)\n",
    "download_if_needed(eye_cascade_filename)\n",
    "download_if_needed(smile_cascade_filename)\n",
    "\n",
    "face_cascade = load_cascade(face_cascade_filename)\n",
    "eye_cascade = load_cascade(eye_cascade_filename)\n",
    "smile_cascade = load_cascade(smile_cascade_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ef121",
   "metadata": {},
   "source": [
    "# Media pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b28954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d03845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cam_index = 0\n",
    "\n",
    "smile_thresh_geo = 0.028\n",
    "smile_thresh_mar = 0.34\n",
    "yawn_mar_thresh = 0.52\n",
    "lift_thresh = 0.05\n",
    "\n",
    "slouch_thresh_z = -0.5      \n",
    "shoulder_tilt_thresh = 0.07 \n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "min_vis = 0.6  \n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.8,\n",
    "    min_tracking_confidence=0.8\n",
    ")\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "LM_MOUTH_LEFT = 61\n",
    "LM_MOUTH_RIGHT = 291\n",
    "LM_UPPER_INNER = 13\n",
    "LM_LOWER_INNER = 14\n",
    "LM_INNER_LEFT = 78\n",
    "LM_INNER_RIGHT = 308\n",
    "LM_LEFT_EYE_UPPER = 159\n",
    "LM_LEFT_EYE_LOWER = 145\n",
    "LM_LEFT_EYE_INNER = 133\n",
    "LM_LEFT_EYE_OUTER = 33\n",
    "LM_RIGHT_EYE_UPPER = 386\n",
    "LM_RIGHT_EYE_LOWER = 374\n",
    "LM_RIGHT_EYE_INNER = 362\n",
    "LM_RIGHT_EYE_OUTER = 263\n",
    "\n",
    "LM_FACE_LEFT_EAR = 234\n",
    "LM_FACE_RIGHT_EAR = 454\n",
    "\n",
    "POSE_LEFT_SHOULDER = 11\n",
    "POSE_RIGHT_SHOULDER = 12\n",
    "POSE_LEFT_ELBOW = 13\n",
    "POSE_RIGHT_ELBOW = 14\n",
    "POSE_LEFT_WRIST = 15\n",
    "POSE_RIGHT_WRIST = 16\n",
    "POSE_LEFT_HIP = 23\n",
    "POSE_RIGHT_HIP = 24\n",
    "POSE_LEFT_EAR = 7    \n",
    "POSE_RIGHT_EAR = 8  \n",
    "\n",
    "def to_px(landmark, w, h):\n",
    "    return np.array([landmark.x * w, landmark.y * h], dtype=np.float32)\n",
    "\n",
    "def norm_point_line_distance(p, a, b):\n",
    "    ab = b - a\n",
    "    ap = p - a\n",
    "    cross = np.abs(np.cross(ab, ap))\n",
    "    denom = np.linalg.norm(ab) + 1e-8\n",
    "    return (cross / denom) / (denom + 1e-8)\n",
    "\n",
    "def mouth_mar(landmarks, w, h):\n",
    "    A = to_px(landmarks[LM_UPPER_INNER], w, h)\n",
    "    B = to_px(landmarks[LM_LOWER_INNER], w, h)\n",
    "    L = to_px(landmarks[LM_INNER_LEFT], w, h)\n",
    "    R = to_px(landmarks[LM_INNER_RIGHT], w, h)\n",
    "    height = np.linalg.norm(B - A)\n",
    "    width = np.linalg.norm(R - L) + 1e-8\n",
    "    return height / width\n",
    "\n",
    "def mouth_corner_lift(landmarks, w, h):\n",
    "    L = to_px(landmarks[LM_MOUTH_LEFT], w, h)\n",
    "    R = to_px(landmarks[LM_MOUTH_RIGHT], w, h)\n",
    "    C_up = to_px(landmarks[LM_UPPER_INNER], w, h)\n",
    "    C_dn = to_px(landmarks[LM_LOWER_INNER], w, h)\n",
    "    C = (C_up + C_dn) / 2.0\n",
    "    corner_y = (L[1] + R[1]) / 2.0\n",
    "    width = np.linalg.norm(R - L) + 1e-8\n",
    "    return (C[1] - corner_y) / width\n",
    "\n",
    "def smile_geo_metric(landmarks, w, h):\n",
    "    L = to_px(landmarks[LM_MOUTH_LEFT], w, h)\n",
    "    R = to_px(landmarks[LM_MOUTH_RIGHT], w, h)\n",
    "    C = to_px(landmarks[LM_UPPER_INNER], w, h)\n",
    "    return norm_point_line_distance(C, L, R)\n",
    "\n",
    "def are_eyes_open(landmarks, w, h, threshold=0.2):\n",
    "    left_v = np.linalg.norm(to_px(landmarks[LM_LEFT_EYE_LOWER], w, h) - to_px(landmarks[LM_LEFT_EYE_UPPER], w, h))\n",
    "    left_h = np.linalg.norm(to_px(landmarks[LM_LEFT_EYE_OUTER], w, h) - to_px(landmarks[LM_LEFT_EYE_INNER], w, h))\n",
    "    right_v = np.linalg.norm(to_px(landmarks[LM_RIGHT_EYE_LOWER], w, h) - to_px(landmarks[LM_RIGHT_EYE_UPPER], w, h))\n",
    "    right_h = np.linalg.norm(to_px(landmarks[LM_RIGHT_EYE_OUTER], w, h) - to_px(landmarks[LM_RIGHT_EYE_INNER], w, h))\n",
    "    left_ear = left_v / (left_h + 1e-8)\n",
    "    right_ear = right_v / (right_h + 1e-8)\n",
    "    return ((left_ear + right_ear) / 2.0) > threshold\n",
    "\n",
    "def check_posture(pose_lms, face_lms, w, h):\n",
    "    if pose_lms is None or face_lms is None:\n",
    "        return \"UNKNOWN\", (200, 200, 200)\n",
    "\n",
    "    p_lms = pose_lms.landmark\n",
    "    \n",
    "    left_shoulder = p_lms[POSE_LEFT_SHOULDER]\n",
    "    right_shoulder = p_lms[POSE_RIGHT_SHOULDER]\n",
    "    left_ear = face_lms[LM_FACE_LEFT_EAR]\n",
    "    right_ear = face_lms[LM_FACE_RIGHT_EAR]\n",
    "\n",
    "    if (left_shoulder.visibility < min_vis or right_shoulder.visibility < min_vis):\n",
    "        return \"UNKNOWN\", (200, 200, 200)\n",
    "\n",
    "    shoulder_y_diff = abs(left_shoulder.y - right_shoulder.y)\n",
    "    if shoulder_y_diff > shoulder_tilt_thresh:\n",
    "        return \"TILTED\", (0, 165, 255)\n",
    "\n",
    "    avg_shoulder_z = (left_shoulder.z + right_shoulder.z) / 2.0\n",
    "    avg_ear_z = (left_ear.z + right_ear.z) / 2.0\n",
    "    z_diff = avg_shoulder_z - avg_ear_z\n",
    "    \n",
    "    if z_diff < slouch_thresh_z:\n",
    "        return \"SLOUCHED\", (0, 0, 255)\n",
    "\n",
    "    return \"GOOD\", (0, 255, 0)\n",
    "\n",
    "def is_hand_raised(pose_lms, left=True):\n",
    "    if pose_lms is None: \n",
    "        return False\n",
    "    lms = pose_lms.landmark\n",
    "    \n",
    "    sh_idx, el_idx, wr_idx = (POSE_LEFT_SHOULDER, POSE_LEFT_ELBOW, POSE_LEFT_WRIST) if left else (POSE_RIGHT_SHOULDER, POSE_RIGHT_ELBOW, POSE_RIGHT_WRIST)\n",
    "    \n",
    "    sh, el, wr = lms[sh_idx], lms[el_idx], lms[wr_idx]\n",
    "    \n",
    "    if (sh.visibility < min_vis) or (el.visibility < min_vis) or (wr.visibility < min_vis):\n",
    "        return False\n",
    "    \n",
    "    return (wr.y < el.y) and (el.y < sh.y - 0.03)\n",
    "\n",
    "clap_state = 'APART'\n",
    "clap_count = 0\n",
    "clap_distance_threshold = 0.15\n",
    "\n",
    "def detect_clap(pose_lms, current_clap_state, current_clap_count):\n",
    "    if pose_lms is None: \n",
    "        return 'APART', current_clap_count, False\n",
    "\n",
    "    lms = pose_lms.landmark\n",
    "    left_wrist = lms[POSE_LEFT_WRIST]\n",
    "    right_wrist = lms[POSE_RIGHT_WRIST]\n",
    "    \n",
    "    if (left_wrist.visibility < min_vis) or (right_wrist.visibility < min_vis):\n",
    "        return current_clap_state, current_clap_count, False\n",
    "    \n",
    "    distance = abs(left_wrist.x - right_wrist.x)\n",
    "    clap_detected_this_frame = False\n",
    "    \n",
    "    if distance < clap_distance_threshold:\n",
    "        if current_clap_state == 'APART':\n",
    "            current_clap_count += 1\n",
    "            clap_detected_this_frame = True\n",
    "            new_clap_state = 'TOGETHER'\n",
    "        else:\n",
    "            new_clap_state = 'TOGETHER'\n",
    "    else:\n",
    "        new_clap_state = 'APART'\n",
    "        \n",
    "    return new_clap_state, current_clap_count, clap_detected_this_frame\n",
    "\n",
    "def annotate_frame(frame, face_lms, pose_lms, smile, eyes, hands, posture, claps):\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    cv2.putText(frame, \"SMILE\" if smile else \"NO SMILE\", (10, 60), font, 0.9, (0, 255, 0) if smile else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"LEFT_RAISED: {hands['left']}\", (10, 90), font, 0.7, (0, 255, 0) if hands['left'] else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"RIGHT_RAISED: {hands['right']}\", (10, 115), font, 0.7, (0, 255, 0) if hands['right'] else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, \"EYES: OPEN\" if eyes else \"EYES: CLOSED\", (10, 140), font, 0.7, (0, 255, 0) if eyes else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"POSTURE: {posture[0]}\", (10, 165), font, 0.7, posture[1], 2)\n",
    "    cv2.putText(frame, f\"CLAPS: {claps['count']}\", (10, 190), font, 0.9, (0, 255, 255), 2)\n",
    "    \n",
    "    if face_lms:\n",
    "        L = to_px(face_lms[LM_MOUTH_LEFT], w, h).astype(int)\n",
    "        R = to_px(face_lms[LM_MOUTH_RIGHT], w, h).astype(int)\n",
    "        A = to_px(face_lms[LM_UPPER_INNER], w, h).astype(int)\n",
    "        B = to_px(face_lms[LM_LOWER_INNER], w, h).astype(int)\n",
    "        cv2.line(frame, L, R, (255, 255, 0), 1)\n",
    "        for p, color in [(L, (0,255,255)), (R, (0,255,255)), (A, (0,140,255)), (B, (0,140,255))]:\n",
    "            cv2.circle(frame, p, 3, color, -1)\n",
    "    \n",
    "    if pose_lms:\n",
    "        lms = pose_lms.landmark\n",
    "        pts = {}\n",
    "        pose_indices = [\n",
    "            POSE_LEFT_SHOULDER, POSE_RIGHT_SHOULDER, POSE_LEFT_ELBOW, POSE_RIGHT_ELBOW,\n",
    "            POSE_LEFT_WRIST, POSE_RIGHT_WRIST, POSE_LEFT_HIP, POSE_RIGHT_HIP,\n",
    "            POSE_LEFT_EAR, POSE_RIGHT_EAR \n",
    "        ]\n",
    "        for idx in pose_indices:\n",
    "            if lms[idx].visibility > min_vis:\n",
    "                pts[idx] = (int(lms[idx].x * w), int(lms[idx].y * h))\n",
    "        \n",
    "        colorL = (0, 255, 0) if hands['left'] else (0, 0, 255)\n",
    "        colorR = (0, 255, 0) if hands['right'] else (0, 0, 255)\n",
    "        \n",
    "        connections = [\n",
    "            (POSE_LEFT_SHOULDER, POSE_RIGHT_SHOULDER, posture[1]),\n",
    "            (POSE_LEFT_HIP, POSE_RIGHT_HIP, posture[1]),\n",
    "            (POSE_LEFT_SHOULDER, POSE_LEFT_HIP, posture[1]),\n",
    "            (POSE_RIGHT_SHOULDER, POSE_RIGHT_HIP, posture[1]),\n",
    "            (POSE_LEFT_SHOULDER, POSE_LEFT_ELBOW, colorL),\n",
    "            (POSE_LEFT_ELBOW, POSE_LEFT_WRIST, colorL),\n",
    "            (POSE_RIGHT_SHOULDER, POSE_RIGHT_ELBOW, colorR),\n",
    "            (POSE_RIGHT_ELBOW, POSE_RIGHT_WRIST, colorR),\n",
    "            (POSE_LEFT_WRIST, POSE_RIGHT_WRIST, (0, 255, 255) if claps['detected'] else (200, 200, 200))\n",
    "        ]\n",
    "        \n",
    "        for conn in connections:\n",
    "            if conn[0] in pts and conn[1] in pts:\n",
    "                cv2.line(frame, pts[conn[0]], pts[conn[1]], conn[2], 2)\n",
    "        \n",
    "        for idx, pt in pts.items():\n",
    "            cv2.circle(frame, pt, 4, (255, 255, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c6c651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(cam_index)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Не удалось открыть камеру\")\n",
    "\n",
    "fps_time = time.time()\n",
    "clap_state = 'APART'\n",
    "clap_count = 0\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: \n",
    "        break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    smiling, eyes_open = False, False\n",
    "    face_res = face_mesh.process(rgb)\n",
    "    face_lms = face_res.multi_face_landmarks[0].landmark if face_res.multi_face_landmarks else None\n",
    "    \n",
    "    if face_lms:\n",
    "        mar = mouth_mar(face_lms, w, h)\n",
    "        lift = mouth_corner_lift(face_lms, w, h)\n",
    "        d = smile_geo_metric(face_lms, w, h)\n",
    "        eyes_open = are_eyes_open(face_lms, w, h)\n",
    "        smiling_geo = (d > smile_thresh_geo) and (lift > lift_thresh) and (mar < yawn_mar_thresh)\n",
    "        smiling_mar = (mar > smile_thresh_mar) and (lift > lift_thresh)\n",
    "        smiling = smiling_geo or smiling_mar\n",
    "\n",
    "    pose_res = pose.process(rgb)\n",
    "    pose_lms = pose_res.pose_landmarks if pose_res and pose_res.pose_landmarks else None\n",
    "    \n",
    "    left_raised = is_hand_raised(pose_lms, left=True)\n",
    "    right_raised = is_hand_raised(pose_lms, left=False)\n",
    "    \n",
    "    posture_result = check_posture(pose_lms, face_lms, w, h)\n",
    "    \n",
    "    clap_state, clap_count, clap_detected = detect_clap(pose_lms, clap_state, clap_count)\n",
    "\n",
    "    annotate_frame(frame, face_lms, pose_lms, smiling, eyes_open, \n",
    "                   {'left': left_raised, 'right': right_raised},\n",
    "                   posture_result,\n",
    "                   {'count': clap_count, 'detected': clap_detected})\n",
    "    \n",
    "    now = time.time()\n",
    "    fps = 1.0 / max(now - fps_time, 1e-6)\n",
    "    fps_time = now\n",
    "    cv2.putText(frame, f\"FPS:{fps:.1f}\", (10, h - 10), font, 0.6, (180, 180, 180), 1)\n",
    "\n",
    "    cv2.imshow(\"Smile + Hands + Claps + Eyes + Posture\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5fb91",
   "metadata": {},
   "source": [
    "# DLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee9df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import bz2\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33eb2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CAM_INDEX = 0\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "MIN_VISIBILITY = 0.6\n",
    "\n",
    "def download_predictor(dat_name=\"shape_predictor_68_face_landmarks.dat\"):\n",
    "    \"\"\"Скачивает и распаковывает модель dlib, если она отсутствует.\"\"\"\n",
    "    bz2_name = dat_name + \".bz2\"\n",
    "    url = \"http://dlib.net/files/\" + bz2_name\n",
    "    if os.path.exists(dat_name):\n",
    "        return True\n",
    "    if not os.path.exists(bz2_name):\n",
    "        try:\n",
    "            print(f\"Скачивает {bz2_name}...\")\n",
    "            urllib.request.urlretrieve(url, bz2_name)\n",
    "            print(\"Файл скачан.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Не удалось скачать: {e}\")\n",
    "            return False\n",
    "    try:\n",
    "        with bz2.BZ2File(bz2_name, 'rb') as f_in, open(dat_name, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "        print(\"Распаковка завершена.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка распаковки: {e}\")\n",
    "        return False\n",
    "\n",
    "if not download_predictor():\n",
    "    raise SystemExit(\"КРИТИЧЕСКАЯ ОШИБКА: не удалось загрузить модель dlib.\")\n",
    "\n",
    "dlib_detector = dlib.get_frontal_face_detector()\n",
    "dlib_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape):\n",
    "\n",
    "    return np.array([(p.x, p.y) for p in shape.parts()], dtype=np.float32)\n",
    "\n",
    "def are_eyes_open(lms, threshold=0.2):\n",
    "\n",
    "    if lms is None: return False\n",
    "\n",
    "    left_v = np.linalg.norm(lms[37] - lms[41])\n",
    "    left_h = np.linalg.norm(lms[36] - lms[39])\n",
    "    left_ear = left_v / (left_h + 1e-8)\n",
    "\n",
    "    right_v = np.linalg.norm(lms[43] - lms[47])\n",
    "    right_h = np.linalg.norm(lms[45] - lms[42])\n",
    "    right_ear = right_v / (right_h + 1e-8)\n",
    "\n",
    "    return (left_ear > threshold) and (right_ear > threshold)\n",
    "\n",
    "def is_hand_raised(pose_lms, left=True):\n",
    "    if pose_lms is None: return False\n",
    "    lms = pose_lms.landmark\n",
    "    sh, el, wr = (11, 13, 15) if left else (12, 14, 16)\n",
    "    \n",
    "    if not all(lms[i].visibility > MIN_VISIBILITY for i in [sh, el, wr]):\n",
    "        return False\n",
    "        \n",
    " \n",
    "class SmileDetector:\n",
    "    def __init__(self, smooth=5, base=25):\n",
    "        self.metrics = {k: deque(maxlen=smooth) for k in ['d', 'lift', 'mar', 'mwr']}\n",
    "        self.base_metrics = {k: deque(maxlen=base) for k in ['d', 'lift', 'mwr']}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _median(q): return np.median(np.array(q)) if q else 0.0\n",
    "\n",
    "    def update(self, lms):\n",
    "        if lms is None: return False\n",
    "        \n",
    "        mouth_width = np.linalg.norm(lms[48] - lms[54]) + 1e-6\n",
    "        d = np.abs(np.cross(lms[54]-lms[48], lms[62]-lms[48])) / mouth_width\n",
    "       \n",
    "        lift = ((lms[62][1]+lms[66][1])/2) - ((lms[48][1]+lms[54][1])/2)\n",
    "\n",
    "        mar = np.linalg.norm(lms[62]-lms[66]) / (np.linalg.norm(lms[60]-lms[64])+1e-6)\n",
    "\n",
    "        mwr = mouth_width / (np.linalg.norm(lms[36]-lms[45])+1e-6)\n",
    "\n",
    "        for k, v in zip(self.metrics.keys(), [d, lift, mar, mwr]): self.metrics[k].append(v)\n",
    "        smoothed = {k: self._median(v) for k, v in self.metrics.items()}\n",
    "\n",
    "        if smoothed['mar'] < 0.4 and smoothed['lift'] > -10:\n",
    "            for k in self.base_metrics.keys(): self.base_metrics[k].append(smoothed[k])\n",
    "        \n",
    "        base = {k: self._median(v) for k, v in self.base_metrics.items()}\n",
    "        \n",
    "        if smoothed['mar'] > 0.8: return False # Анти-зевок\n",
    "\n",
    "        if len(self.base_metrics) < 10: return (smoothed['lift'] > 2) and (smoothed['mwr'] > 0.6)\n",
    "        \n",
    "        hits = (smoothed['d'] > base['d'] + 1.5) + \\\n",
    "               (smoothed['lift'] > base['lift'] + 2.0) + \\\n",
    "               (smoothed['mwr'] > base['mwr'] * 1.05)\n",
    "        \n",
    "        return hits >= 2\n",
    "\n",
    "class PostureDetector:\n",
    "    def __init__(self, buffer_size=10, slouch_thresh=0.3, tilt_thresh=0.07):\n",
    "        self.z_diff_buffer = deque(maxlen=buffer_size)\n",
    "        self.slouch_thresh, self.tilt_thresh = slouch_thresh, tilt_thresh\n",
    "    \n",
    "    def update(self, pose_lms):\n",
    "        if pose_lms is None or not all(pose_lms.landmark[i].visibility > MIN_VISIBILITY for i in [7,8,11,12]):\n",
    "            self.z_diff_buffer.clear()\n",
    "            return \"UNKNOWN\", (200, 200, 200), 0.0\n",
    "\n",
    "        p_lms = pose_lms.landmark\n",
    "        z_diff = ((p_lms[11].z + p_lms[12].z)/2) - ((p_lms[7].z + p_lms[8].z)/2)\n",
    "        self.z_diff_buffer.append(z_diff)\n",
    "        \n",
    "        if len(self.z_diff_buffer) < self.z_diff_buffer.maxlen / 2:\n",
    "            return \"WAITING...\", (200, 200, 200), 0.0\n",
    "            \n",
    "        smoothed_z = np.median(self.z_diff_buffer)\n",
    "\n",
    "        if abs(p_lms[11].y - p_lms[12].y) > self.tilt_thresh:\n",
    "            return \"TILTED\", (0, 165, 255), smoothed_z\n",
    "        \n",
    "        if smoothed_z > self.slouch_thresh:\n",
    "            return \"SLOUCHED\", (0, 0, 255), smoothed_z\n",
    "            \n",
    "        return \"GOOD\", (0, 255, 0), smoothed_z\n",
    "\n",
    "class ClapDetector:\n",
    "    def __init__(self, distance_px=60, cooldown=0.3):\n",
    "        self.distance_px, self.cooldown = distance_px, cooldown\n",
    "        self.state_open = True\n",
    "        self.last_event_t = 0.0\n",
    "    \n",
    "    def update(self, pose_lms, w, h):\n",
    "        now = time.time()\n",
    "        if pose_lms is None or not all(pose_lms.landmark[i].visibility > MIN_VISIBILITY for i in [15,16]):\n",
    "            return False\n",
    "            \n",
    "        lms = pose_lms.landmark\n",
    "        d_px = np.linalg.norm(np.array([(lms[16].x-lms[15].x)*w, (lms[16].y-lms[15].y)*h]))\n",
    "        hands_close = d_px < self.distance_px\n",
    "        \n",
    "        event = False\n",
    "        if self.state_open and hands_close and (now - self.last_event_t > self.cooldown):\n",
    "            event, self.state_open, self.last_event_t = True, False, now\n",
    "        \n",
    "        if not hands_close:\n",
    "            self.state_open = True\n",
    "            \n",
    "        return event\n",
    "\n",
    "\n",
    "def draw_landmarks(frame, face_lms, pose_lms, posture_color):\n",
    "    if face_lms is not None:\n",
    "        for i in range(68):\n",
    "            cv2.circle(frame, tuple(face_lms[i].astype(int)), 1, (0, 255, 0), -1)\n",
    "            \n",
    "    if pose_lms is not None:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            frame, pose_lms, mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp.solutions.drawing_styles.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.DrawingSpec(color=posture_color, thickness=2, circle_radius=2)\n",
    "        )\n",
    "\n",
    "def draw_final_status(frame, fps, states):\n",
    "    h, _ = frame.shape[:2]\n",
    "    cv2.putText(frame, f\"FPS:{fps:.1f}\", (10, h - 10), FONT, 0.6, (180, 180, 180), 1)\n",
    "    \n",
    "    posture_status, posture_color = states['posture']\n",
    "    posture_text = f\"POSTURE: {posture_status} (Z: {states.get('z_diff',0.0):.2f})\"\n",
    "    \n",
    "    cv2.putText(frame, \"SMILE\" if states['smiling'] else \"NO SMILE\", (10,30), FONT, 0.7, (0,255,0) if states['smiling'] else (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"EYES: {'OPEN' if states['eyes_open'] else 'CLOSED'}\", (10,55), FONT, 0.7, (0,255,0) if states['eyes_open'] else (0,0,255), 2)\n",
    "    cv2.putText(frame, posture_text, (10, 80), FONT, 0.7, posture_color, 2)\n",
    "    cv2.putText(frame, f\"L.HAND: {'UP' if states['left_raised'] else 'DOWN'}\", (10,105), FONT, 0.7, (0,255,0) if states['left_raised'] else (200,200,200), 2)\n",
    "    cv2.putText(frame, f\"R.HAND: {'UP' if states['right_raised'] else 'DOWN'}\", (10,130), FONT, 0.7, (0,255,0) if states['right_raised'] else (200,200,200), 2)\n",
    "    cv2.putText(frame, f\"CLAPS: {states['clap_count']}\", (10,155), FONT, 0.7, (0,255,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92bf06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(CAM_INDEX)\n",
    "posture_det = PostureDetector(slouch_thresh=0.3)\n",
    "smile_det = SmileDetector()\n",
    "clap_det = ClapDetector()\n",
    "clap_count = 0\n",
    "fps_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    pose_results = pose_detector.process(rgb_frame)\n",
    "    pose_landmarks = pose_results.pose_landmarks\n",
    "    dlib_faces = dlib_detector(rgb_frame, 0)\n",
    "    face_landmarks = shape_to_np(dlib_predictor(rgb_frame, dlib_faces[0])) if dlib_faces else None\n",
    "\n",
    "    states = {}\n",
    "    posture_status, posture_color, z_diff = posture_det.update(pose_landmarks)\n",
    "    states['posture'], states['z_diff'] = (posture_status, posture_color), z_diff\n",
    "    states['smiling'] = smile_det.update(face_landmarks)\n",
    "    states['eyes_open'] = are_eyes_open(face_landmarks)\n",
    "    states['left_raised'], states['right_raised'] = is_hand_raised(pose_landmarks,True), is_hand_raised(pose_landmarks,False)\n",
    "    if clap_det.update(pose_landmarks, w, h): clap_count += 1\n",
    "    states['clap_count'] = clap_count\n",
    "    \n",
    "    draw_landmarks(frame, face_landmarks, pose_landmarks, posture_color)\n",
    "    now = time.time(); fps = 1.0/max(now-fps_time, 1e-6); fps_time = now\n",
    "    draw_final_status(frame, fps, states)\n",
    "    \n",
    "    cv2.imshow(\"Multi-Event Detector\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27: break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cab4d2",
   "metadata": {},
   "source": [
    "# MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "665a0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FPT = False\n",
    "FPT_MTCNN = None\n",
    "IPAZC_MTCNN = None\n",
    "\n",
    "try:\n",
    "    from facenet_pytorch import MTCNN as FPT_MTCNN\n",
    "    USE_FPT = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from mtcnn.mtcnn import MTCNN as IPAZC_MTCNN\n",
    "except Exception:\n",
    "    IPAZC_MTCNN = None\n",
    "\n",
    "def init_mtcnn():\n",
    "    fpt = FPT_MTCNN(keep_all=True) if (USE_FPT and FPT_MTCNN is not None) else None\n",
    "    ipz = IPAZC_MTCNN() if (IPAZC_MTCNN is not None) else None\n",
    "    return fpt, ipz\n",
    "\n",
    "\n",
    "def mtcnn_detect(fpt, ipz, frame_bgr):\n",
    "    dets = []\n",
    "    if fpt is not None:\n",
    "        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        boxes, probs, landmarks = fpt.detect(rgb, landmarks=True)\n",
    "        if boxes is not None:\n",
    "            for i, b in enumerate(boxes):\n",
    "                if probs is None or probs[i] is None:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = b.astype(int)\n",
    "                w = int(x2 - x1); h = int(y2 - y1)\n",
    "                lm = landmarks[i]\n",
    "                kp = {\n",
    "                    \"left_eye\":   (int(lm[0][0]), int(lm[0][1])),\n",
    "                    \"right_eye\":  (int(lm[1][0]), int(lm[1][1])),\n",
    "                    \"nose\":       (int(lm[2][0]), int(lm[2][1])),\n",
    "                    \"mouth_left\": (int(lm[3][0]), int(lm[3][1])),\n",
    "                    \"mouth_right\":(int(lm[4][0]), int(lm[4][1])),\n",
    "                }\n",
    "                dets.append({\"box\": [int(x1), int(y1), w, h], \"keypoints\": kp, \"confidence\": float(probs[i])})\n",
    "    if (not dets) and (ipz is not None):\n",
    "        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        raw = ipz.detect_faces(rgb)\n",
    "        for r in raw:\n",
    "            x, y, w, h = r[\"box\"]\n",
    "            kp = r.get(\"keypoints\", {})\n",
    "            dets.append({\"box\": [int(x), int(y), int(w), int(h)],\n",
    "                         \"keypoints\": {k: (int(v[0]), int(v[1])) for k, v in kp.items()},\n",
    "                         \"confidence\": float(r.get(\"confidence\", 0.0))})\n",
    "    return dets\n",
    "\n",
    "def draw_mtcnn_detections(frame, detections):\n",
    "    for det in detections:\n",
    "        x, y, w, h = det[\"box\"]\n",
    "        kp = det.get(\"keypoints\", {})\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        for name in (\"left_eye\", \"right_eye\", \"nose\", \"mouth_left\", \"mouth_right\"):\n",
    "            if name in kp and kp[name] is not None:\n",
    "                cx, cy = int(kp[name][0]), int(kp[name][1])\n",
    "                cv2.circle(frame, (cx, cy), 3, (0, 255, 0), -1)\n",
    "                cv2.putText(frame, name, (cx+4, cy-4), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (50, 255, 50), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "LM_MOUTH_LEFT = 61; LM_MOUTH_RIGHT = 291; LM_UPPER_INNER = 13; LM_LOWER_INNER = 14\n",
    "LM_INNER_LEFT = 78; LM_INNER_RIGHT = 308\n",
    "LM_LEFT_EYE_UPPER = 159; LM_LEFT_EYE_LOWER = 145; LM_LEFT_EYE_INNER = 133; LM_LEFT_EYE_OUTER = 33\n",
    "LM_RIGHT_EYE_UPPER = 386; LM_RIGHT_EYE_LOWER = 374; LM_RIGHT_EYE_INNER = 362; LM_RIGHT_EYE_OUTER = 263\n",
    "\n",
    "def to_px(landmark, w, h):\n",
    "    return np.array([landmark.x * w, landmark.y * h], dtype=np.float32)\n",
    "\n",
    "def mouth_mar(lms, w, h):\n",
    "    A = to_px(lms[LM_UPPER_INNER], w, h)\n",
    "    B = to_px(lms[LM_LOWER_INNER], w, h)\n",
    "    L = to_px(lms[LM_INNER_LEFT], w, h)\n",
    "    R = to_px(lms[LM_INNER_RIGHT], w, h)\n",
    "    height = np.linalg.norm(B - A)\n",
    "    width = np.linalg.norm(R - L) + 1e-8\n",
    "    return float(height / width)\n",
    "\n",
    "def mouth_corner_lift(lms, w, h):\n",
    "    L = to_px(lms[LM_MOUTH_LEFT], w, h)\n",
    "    R = to_px(lms[LM_MOUTH_RIGHT], w, h)\n",
    "    C_up = to_px(lms[LM_UPPER_INNER], w, h)\n",
    "    C_dn = to_px(lms[LM_LOWER_INNER], w, h)\n",
    "    C = (C_up + C_dn) / 2.0\n",
    "    corner_y = (L[1] + R[1]) / 2.0\n",
    "    width = np.linalg.norm(R - L) + 1e-8\n",
    "    return float((C[1] - corner_y) / width)\n",
    "\n",
    "def smile_geo_metric(lms, w, h):\n",
    "    L = to_px(lms[LM_MOUTH_LEFT], w, h)\n",
    "    R = to_px(lms[LM_MOUTH_RIGHT], w, h)\n",
    "    C = to_px(lms[LM_UPPER_INNER], w, h)\n",
    "    ab = R - L; ap = C - L\n",
    "    cross = np.abs(np.cross(ab, ap))\n",
    "    denom = np.linalg.norm(ab) + 1e-8\n",
    "    return float((cross / denom) / (denom + 1e-8))\n",
    "\n",
    "def is_smile_facemesh(lms, w, h):\n",
    "    d = smile_geo_metric(lms, w, h)\n",
    "    lift = mouth_corner_lift(lms, w, h)\n",
    "    mar = mouth_mar(lms, w, h)\n",
    "    smile_main = (d > 0.016) and (lift > 0.050) and (mar < 0.60)\n",
    "    smile_teeth = (mar > 0.32) and (lift > 0.050)\n",
    "    smiling = bool(smile_main or smile_teeth)\n",
    "    if (mar > 0.70) and (lift < 0.03):\n",
    "        smiling = False\n",
    "    return smiling, {\"d\": d, \"lift\": lift, \"mar\": mar}\n",
    "\n",
    "def are_eyes_open(lms, w, h, threshold=0.2):\n",
    "    left_v = np.linalg.norm(to_px(lms[LM_LEFT_EYE_LOWER], w, h) - to_px(lms[LM_LEFT_EYE_UPPER], w, h))\n",
    "    left_h = np.linalg.norm(to_px(lms[LM_LEFT_EYE_OUTER], w, h) - to_px(lms[LM_LEFT_EYE_INNER], w, h))\n",
    "    left_ear = left_v / (left_h + 1e-8)\n",
    "    \n",
    "    right_v = np.linalg.norm(to_px(lms[LM_RIGHT_EYE_LOWER], w, h) - to_px(lms[LM_RIGHT_EYE_UPPER], w, h))\n",
    "    right_h = np.linalg.norm(to_px(lms[LM_RIGHT_EYE_OUTER], w, h) - to_px(lms[LM_RIGHT_EYE_INNER], w, h))\n",
    "    right_ear = right_v / (right_h + 1e-8)\n",
    "    \n",
    "    return (left_ear > threshold) and (right_ear > threshold)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    model_complexity=2,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "POSE_LEFT_SHOULDER = 11; POSE_RIGHT_SHOULDER = 12\n",
    "POSE_LEFT_ELBOW = 13; POSE_RIGHT_ELBOW = 14\n",
    "POSE_LEFT_WRIST = 15; POSE_RIGHT_WRIST = 16\n",
    "POSE_LEFT_EAR = 7; POSE_RIGHT_EAR = 8\n",
    "\n",
    "raise_margin = 0.03\n",
    "min_vis = 0.6\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "class PostureDetector:\n",
    "    def __init__(self, buffer_size=10, slouch_thresh=0.3, tilt_thresh=0.07):\n",
    "        self.z_diff_buffer = deque(maxlen=buffer_size)\n",
    "        self.slouch_thresh = slouch_thresh\n",
    "        self.tilt_thresh = tilt_thresh\n",
    "    \n",
    "    def update(self, pose_lms):\n",
    "        if pose_lms is None or not all(pose_lms.landmark[i].visibility > min_vis for i in [7,8,11,12]):\n",
    "            self.z_diff_buffer.clear()\n",
    "            return \"UNKNOWN\", (200, 200, 200), 0.0\n",
    "        \n",
    "        p_lms = pose_lms.landmark\n",
    "        z_diff = ((p_lms[11].z + p_lms[12].z)/2) - ((p_lms[7].z + p_lms[8].z)/2)\n",
    "        self.z_diff_buffer.append(z_diff)\n",
    "        \n",
    "        if len(self.z_diff_buffer) < self.z_diff_buffer.maxlen / 2:\n",
    "            return \"WAITING...\", (200, 200, 200), 0.0\n",
    "        \n",
    "        smoothed_z = np.median(self.z_diff_buffer)\n",
    "        \n",
    "        if abs(p_lms[11].y - p_lms[12].y) > self.tilt_thresh:\n",
    "            return \"TILTED\", (0, 165, 255), smoothed_z\n",
    "        \n",
    "        if smoothed_z > self.slouch_thresh:\n",
    "            return \"SLOUCHED\", (0, 0, 255), smoothed_z\n",
    "        \n",
    "        return \"GOOD\", (0, 255, 0), smoothed_z\n",
    "\n",
    "def pose_point_xy(lm, w, h):\n",
    "    return np.array([lm.x * w, lm.y * h], dtype=np.float32)\n",
    "\n",
    "def both_vis_ok(pose_lms, idx_a, idx_b):\n",
    "    la = pose_lms.landmark[idx_a]; lb = pose_lms.landmark[idx_b]\n",
    "    return (la.visibility >= min_vis) and (lb.visibility >= min_vis)\n",
    "\n",
    "def is_hand_raised(pose_lms, left=True):\n",
    "    if pose_lms is None:\n",
    "        return False\n",
    "    sh_idx, wr_idx = (POSE_LEFT_SHOULDER, POSE_LEFT_WRIST) if left else (POSE_RIGHT_SHOULDER, POSE_RIGHT_WRIST)\n",
    "    if not both_vis_ok(pose_lms, sh_idx, wr_idx):\n",
    "        return False\n",
    "    sh = pose_lms.landmark[sh_idx]; wr = pose_lms.landmark[wr_idx]\n",
    "    return wr.y < (sh.y - raise_margin)\n",
    "\n",
    "def shoulders_width_norm(pose_lms, w, h):\n",
    "    L = pose_point_xy(pose_lms.landmark[POSE_LEFT_SHOULDER], w, h)\n",
    "    R = pose_point_xy(pose_lms.landmark[POSE_RIGHT_SHOULDER], w, h)\n",
    "    return float(np.linalg.norm(R - L) + 1e-8)\n",
    "\n",
    "def wrists_distance_px(pose_lms, w, h):\n",
    "    L = pose_point_xy(pose_lms.landmark[POSE_LEFT_WRIST], w, h)\n",
    "    R = pose_point_xy(pose_lms.landmark[POSE_RIGHT_WRIST], w, h)\n",
    "    return float(np.linalg.norm(R - L))\n",
    "\n",
    "class ClapDetector:\n",
    "    def __init__(self, close_thresh=0.30, open_thresh=0.48, min_drop_window=0.12, \n",
    "                 abs_close_px=90, abs_open_px=160, win=6, cooldown=0.35):\n",
    "        self.close_thresh, self.open_thresh = close_thresh, open_thresh\n",
    "        self.min_drop_window, self.abs_close_px = min_drop_window, abs_close_px\n",
    "        self.abs_open_px, self.cooldown = abs_open_px, cooldown\n",
    "        self.d_hist = deque(maxlen=win)\n",
    "        self.state_open = True\n",
    "        self.last_event_t = 0.0\n",
    "\n",
    "    def update(self, pose_lms, w, h):\n",
    "        now = time.time()\n",
    "        event = False\n",
    "        d_norm = float('nan'); drop = 0.0; hands_close = False\n",
    "        if (pose_lms is None) or (not (both_vis_ok(pose_lms, POSE_LEFT_WRIST, POSE_RIGHT_WRIST) and\n",
    "                                       both_vis_ok(pose_lms, POSE_LEFT_SHOULDER, POSE_RIGHT_SHOULDER))):\n",
    "            self.d_hist.clear()\n",
    "            return event, {\"d_norm\": d_norm, \"hands_close\": hands_close, \"delta\": drop}\n",
    "        \n",
    "        sw = shoulders_width_norm(pose_lms, w, h)\n",
    "        d_px = wrists_distance_px(pose_lms, w, h)\n",
    "        d_norm = (d_px / sw) if sw > 0 else float('nan')\n",
    "\n",
    "        prev_vals = list(self.d_hist)\n",
    "        prev_max = (max(prev_vals) if prev_vals else d_norm)\n",
    "        drop = ((prev_max - d_norm) if (not np.isnan(d_norm) and not np.isnan(prev_max)) else 0.0)\n",
    "\n",
    "        self.d_hist.append(d_norm if not np.isnan(d_norm) else 0.0)\n",
    "\n",
    "        hands_close = ((not np.isnan(d_norm) and d_norm < self.close_thresh) or (d_px < self.abs_close_px))\n",
    "        open_before = ((not np.isnan(prev_max) and prev_max > self.open_thresh) or (d_px > self.abs_open_px))\n",
    "\n",
    "        cond_norm = ((not np.isnan(d_norm)) and (d_norm < self.close_thresh) and (drop > self.min_drop_window) and open_before)\n",
    "        cond_abs = ((d_px < self.abs_close_px) and (drop > (self.min_drop_window * 0.6)) and open_before)\n",
    "\n",
    "        if self.state_open and (cond_norm or cond_abs) and ((now - self.last_event_t) > self.cooldown):\n",
    "            event, self.state_open, self.last_event_t = True, False, now\n",
    "\n",
    "        if ((not np.isnan(d_norm) and d_norm > self.open_thresh) or (d_px > self.abs_open_px)):\n",
    "            self.state_open = True\n",
    "        \n",
    "        return event, {\"d_norm\": float(d_norm), \"hands_close\": bool(hands_close), \"delta\": float(drop)}\n",
    "\n",
    "def annotate_pose(frame, pose_lms, left_raised, right_raised):\n",
    "    if pose_lms is None:\n",
    "        return\n",
    "    h, w = frame.shape[:2]\n",
    "    lms = pose_lms.landmark\n",
    "    pts = {}\n",
    "    for idx in [POSE_LEFT_SHOULDER, POSE_RIGHT_SHOULDER, POSE_LEFT_ELBOW, POSE_RIGHT_ELBOW, \n",
    "                POSE_LEFT_WRIST, POSE_RIGHT_WRIST]:\n",
    "        if lms[idx].visibility > min_vis:\n",
    "            x = int(lms[idx].x * w); y = int(lms[idx].y * h); pts[idx] = (x, y)\n",
    "    \n",
    "    colorL = (0, 255, 0) if left_raised else (0, 0, 255)\n",
    "    colorR = (0, 255, 0) if right_raised else (0, 0, 255)\n",
    "    \n",
    "    if POSE_LEFT_SHOULDER in pts: cv2.circle(frame, pts[POSE_LEFT_SHOULDER], 4, (255, 255, 0), -1)\n",
    "    if POSE_RIGHT_SHOULDER in pts: cv2.circle(frame, pts[POSE_RIGHT_SHOULDER], 4, (255, 255, 0), -1)\n",
    "    \n",
    "    if POSE_LEFT_ELBOW in pts: cv2.circle(frame, pts[POSE_LEFT_ELBOW], 4, (200, 200, 255), -1)\n",
    "    if POSE_RIGHT_ELBOW in pts: cv2.circle(frame, pts[POSE_RIGHT_ELBOW], 4, (200, 200, 255), -1)\n",
    "    \n",
    "    if POSE_LEFT_WRIST in pts: cv2.circle(frame, pts[POSE_LEFT_WRIST], 5, colorL, -1)\n",
    "    if POSE_RIGHT_WRIST in pts: cv2.circle(frame, pts[POSE_RIGHT_WRIST], 5, colorR, -1)\n",
    "    \n",
    "    if POSE_LEFT_SHOULDER in pts and POSE_LEFT_ELBOW in pts:\n",
    "        cv2.line(frame, pts[POSE_LEFT_SHOULDER], pts[POSE_LEFT_ELBOW], colorL, 2)\n",
    "    if POSE_LEFT_ELBOW in pts and POSE_LEFT_WRIST in pts:\n",
    "        cv2.line(frame, pts[POSE_LEFT_ELBOW], pts[POSE_LEFT_WRIST], colorL, 2)\n",
    "    \n",
    "    if POSE_RIGHT_SHOULDER in pts and POSE_RIGHT_ELBOW in pts:\n",
    "        cv2.line(frame, pts[POSE_RIGHT_SHOULDER], pts[POSE_RIGHT_ELBOW], colorR, 2)\n",
    "    if POSE_RIGHT_ELBOW in pts and POSE_RIGHT_WRIST in pts:\n",
    "        cv2.line(frame, pts[POSE_RIGHT_ELBOW], pts[POSE_RIGHT_WRIST], colorR, 2)\n",
    "\n",
    "def annotate_clap(frame, pose_lms, w, h, info, event=False):\n",
    "    if pose_lms is None:\n",
    "        return\n",
    "    LW = pose_point_xy(pose_lms.landmark[POSE_LEFT_WRIST], w, h).astype(int)\n",
    "    RW = pose_point_xy(pose_lms.landmark[POSE_RIGHT_WRIST], w, h).astype(int)\n",
    "    color = (0, 255, 0) if info.get(\"hands_close\", False) else (0, 0, 255)\n",
    "    cv2.line(frame, tuple(LW), tuple(RW), color, 3 if info.get(\"hands_close\", False) else 1)\n",
    "    if event:\n",
    "        cv2.circle(frame, ((LW[0] + RW[0]) // 2, (LW[1] + RW[1]) // 2), 12, (0, 255, 255), 2)\n",
    "    def f2(x):\n",
    "        try: return f\"{float(x):.2f}\" if x is not None else \"nan\"\n",
    "        except: return \"nan\"\n",
    "    txt = f\"d_norm={f2(info.get('d_norm'))} drop={f2(info.get('delta'))}\"\n",
    "    cv2.putText(frame, txt, (10, 210), font, 0.6, (200, 255, 200), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e0ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPT initialized: True\n",
      "IPZ initialized: False\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Не удалось открыть камеру\")\n",
    "\n",
    "fpt, ipz = init_mtcnn()\n",
    "print(f\"FPT initialized: {fpt is not None}\")\n",
    "print(f\"IPZ initialized: {ipz is not None}\")\n",
    "\n",
    "clap_det = ClapDetector()\n",
    "posture_det = PostureDetector(slouch_thresh=0.3)\n",
    "clap_count = 0\n",
    "fps_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    pose_res = pose.process(rgb)\n",
    "    pose_lms = pose_res.pose_landmarks if pose_res and pose_res.pose_landmarks else None\n",
    "    \n",
    "    left_raised = is_hand_raised(pose_lms, left=True)\n",
    "    right_raised = is_hand_raised(pose_lms, left=False)\n",
    "    one_hand_raised = bool(left_raised ^ right_raised)\n",
    "    two_hands_raised = bool(left_raised and right_raised)\n",
    "    \n",
    "    posture_status, posture_color, z_diff = posture_det.update(pose_lms)\n",
    "    \n",
    "    annotate_pose(frame, pose_lms, left_raised, right_raised)\n",
    "    \n",
    "    clap_event, clap_info = clap_det.update(pose_lms, w, h)\n",
    "    if clap_event: clap_count += 1\n",
    "    annotate_clap(frame, pose_lms, w, h, clap_info, event=clap_event)\n",
    "\n",
    "    dets = mtcnn_detect(fpt, ipz, frame)\n",
    "    if dets: draw_mtcnn_detections(frame, dets)\n",
    "\n",
    "    smiling = False\n",
    "    eyes_open = False\n",
    "    fm = face_mesh.process(rgb)\n",
    "    if fm.multi_face_landmarks:\n",
    "        lms = fm.multi_face_landmarks[0].landmark\n",
    "        smiling, sc = is_smile_facemesh(lms, w, h)\n",
    "        eyes_open = are_eyes_open(lms, w, h)\n",
    "        \n",
    "        cv2.putText(frame, f\"d={sc['d']:.3f} lift={sc['lift']:.3f} MAR={sc['mar']:.3f}\",\n",
    "                    (10, 30), font, 0.58, (255, 255, 0), 2)\n",
    "        \n",
    "        L = to_px(lms[LM_MOUTH_LEFT], w, h).astype(int)\n",
    "        R = to_px(lms[LM_MOUTH_RIGHT], w, h).astype(int)\n",
    "        A = to_px(lms[LM_UPPER_INNER], w, h).astype(int)\n",
    "        B = to_px(lms[LM_LOWER_INNER], w, h).astype(int)\n",
    "        cv2.line(frame, L, R, (255, 255, 0), 1)\n",
    "        for p, color in [(L, (0, 255, 255)), (R, (0, 255, 255)), (A, (0, 140, 255)), (B, (0, 140, 255))]:\n",
    "            cv2.circle(frame, tuple(p), 3, color, -1)\n",
    "    else:\n",
    "        best = max(dets, key=lambda d: d.get(\"confidence\", 0.0)) if dets else None\n",
    "        if best and \"keypoints\" in best:\n",
    "            le = best[\"keypoints\"].get(\"left_eye\"); re = best[\"keypoints\"].get(\"right_eye\")\n",
    "            ml = best[\"keypoints\"].get(\"mouth_left\"); mr = best[\"keypoints\"].get(\"mouth_right\")\n",
    "            if None not in (le, re, ml, mr):\n",
    "                iod = np.linalg.norm(np.array(le) - np.array(re)) + 1e-8\n",
    "                mwr = np.linalg.norm(np.array(ml) - np.array(mr)) / iod\n",
    "                eye_y = (le[1] + re[1]) / 2.0\n",
    "                corner_y = (ml[1] + mr[1]) / 2.0\n",
    "                lift2 = (eye_y - corner_y) / iod\n",
    "                smiling = (mwr > 0.42) and (lift2 > 0.06)\n",
    "                cv2.putText(frame, f\"[MTCNN] MWR={mwr:.3f} LIFT2={lift2:.3f}\",\n",
    "                            (10, 30), font, 0.58, (200, 255, 200), 2)\n",
    "\n",
    "    cv2.putText(frame, \"SMILE\" if smiling else \"NO SMILE\", (10, 60), font, 0.9, \n",
    "                (0, 255, 0) if smiling else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"EYES: {'OPEN' if eyes_open else 'CLOSED'}\", (10, 85), font, 0.7, \n",
    "                (0, 255, 0) if eyes_open else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"POSTURE: {posture_status} (Z: {z_diff:.2f})\", (10, 110), font, 0.7, \n",
    "                posture_color, 2)\n",
    "    cv2.putText(frame, f\"LEFT_RAISED: {left_raised}\", (10, 135), font, 0.7, \n",
    "                (0, 255, 0) if left_raised else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"RIGHT_RAISED: {right_raised}\", (10, 160), font, 0.7, \n",
    "                (0, 255, 0) if right_raised else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"ONE_HAND_RAISED (XOR): {one_hand_raised}\", (10, 185), font, 0.7, \n",
    "                (0, 255, 0) if one_hand_raised else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"TWO_HANDS_RAISED: {two_hands_raised}\", (10, 210), font, 0.7, \n",
    "                (0, 255, 0) if two_hands_raised else (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"CLAPS: {clap_count}\", (10, 235), font, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "    \n",
    "    now = time.time()\n",
    "    fps = 1.0 / max(now - fps_time, 1e-6)\n",
    "    fps_time = now\n",
    "    cv2.putText(frame, f\"FPS:{fps:.1f}\", (10, h - 10), font, 0.6, (180, 180, 180), 1)\n",
    "\n",
    "    cv2.imshow(\"Hybrid: MTCNN + MediaPipe (Smile+Eyes+Posture+Hands+Clap)\", frame)\n",
    "    if (cv2.waitKey(1) & 0xFF) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb01ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80674c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvstack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
